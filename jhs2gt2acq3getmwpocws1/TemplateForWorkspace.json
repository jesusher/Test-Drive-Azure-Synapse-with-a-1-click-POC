{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "jhs2gt2acq3getmwpocws1"
		},
		"AzureSqlDatabase1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSqlDatabase1'"
		},
		"AzureSynapseAnalytics1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSynapseAnalytics1'"
		},
		"AzureSynapseAnalytics2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureSynapseAnalytics2'"
		},
		"TripFaresSynapseAnalyticsLinkedService_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'TripFaresSynapseAnalyticsLinkedService'"
		},
		"adlsworspace_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'adlsworspace'"
		},
		"linkedService1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'linkedService1'"
		},
		"HttpServerTripDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/trip-data.csv"
		},
		"HttpServerTripFareDataLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/Azure/Test-Drive-Azure-Synapse-with-a-1-click-POC/main/tripDataAndFaresCSV/fares-data.csv"
		},
		"TripFaresDataLakeStorageLinkedService_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().datalakeAccountName,'.dfs.core.windows.net')}"
		},
		"adlsworspace_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://jhs2gt2acq3getmwpoc.dfs.core.windows.net"
		},
		"keyVaultLinkedservice_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "@{concat('https://',linkedService().keyVaultName,'.vault.azure.net/')}"
		},
		"linkedService1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://jhs2gt2acq3getmwpoc.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/CopyDeltaTabletoSQLTable')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "1.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "CopyDeltatoSQL",
								"type": "DataFlowReference",
								"parameters": {
									"adf_tablename": {
										"value": "'@{pipeline().parameters.tablename}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"DeltaTable": {},
									"sink1": {}
								}
							},
							"staging": {
								"linkedService": {
									"referenceName": "linkedService1",
									"type": "LinkedServiceReference"
								},
								"folderPath": "adfstagedpolybasetempdata"
							},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"tablename": {
						"type": "string"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/CopyDeltatoSQL')]",
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "IngestTripDataIntoADLS",
						"description": "Copies the trip data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "tripDataSink",
								"type": "DatasetReference",
								"parameters": {
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "IngestTripFaresDataIntoADLS",
						"description": "Copies the trip fare data csv file from the git repo and loads it into the ADLS.",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.00:10:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "faresDataSink",
								"type": "DatasetReference",
								"parameters": {
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"datalakeAccountName": {
										"value": "@pipeline().parameters.datalakeAccountName",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "JoinAndAggregateData",
						"description": "Reads the raw data from both CSV files inside the ADLS, performs the desired transformations (inner join and aggregation) and writes the transformed data into the synapse SQL pool.",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:30:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "tripFaresDataTransformations",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"TripDataCSV": {
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										}
									},
									"FaresDataCSV": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									},
									"SynapseAnalyticsSink": {
										"SchemaName": {
											"value": "@pipeline().parameters.SchemaName",
											"type": "Expression"
										},
										"SynapseWorkspaceName": {
											"value": "@pipeline().parameters.SynapseWorkspaceName",
											"type": "Expression"
										},
										"SQLDedicatedPoolName": {
											"value": "@pipeline().parameters.SQLDedicatedPoolName",
											"type": "Expression"
										},
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"SQLLoginUsername": {
											"value": "@pipeline().parameters.SQLLoginUsername",
											"type": "Expression"
										}
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					},
					{
						"name": "Create Schema If Does Not Exists",
						"description": "Creates the schema inside the SQL dedicated pool. Shema name comes from the pipeline parameter 'SchemaName'.",
						"type": "Lookup",
						"dependsOn": [
							{
								"activity": "IngestTripDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							},
							{
								"activity": "IngestTripFaresDataIntoADLS",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.00:05:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "SqlDWSource",
								"sqlReaderQuery": {
									"value": "IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '@{pipeline().parameters.SchemaName}')\nBEGIN\nEXEC('CREATE SCHEMA @{pipeline().parameters.SchemaName}')\nselect Count(*) from sys.symmetric_keys;\nEND\nELSE\nBEGIN\n    select Count(*) from sys.symmetric_keys;\nEND",
									"type": "Expression"
								},
								"queryTimeout": "02:00:00",
								"partitionOption": "None"
							},
							"dataset": {
								"referenceName": "azureSynapseAnalyticsSchema",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							},
							"firstRowOnly": false
						}
					},
					{
						"name": "Copy data Trips Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'TripsData'))\nBEGIN \n   Truncate table TripsData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "tripsDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsTripsData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					},
					{
						"name": "Copy data Fares Data",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Create Schema If Does Not Exists",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "HttpReadSettings",
									"requestMethod": "GET"
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "SqlDWSink",
								"preCopyScript": "IF (EXISTS (SELECT *\n  FROM INFORMATION_SCHEMA.TABLES\n  WHERE TABLE_SCHEMA = 'dbo'\n  AND TABLE_NAME = 'FaresData'))\nBEGIN \n   Truncate table FaresData;\nEnd\n",
								"allowPolyBase": true,
								"polyBaseSettings": {
									"rejectValue": 0,
									"rejectType": "value",
									"useTypeDefault": true
								},
								"tableOption": "autoCreate",
								"disableMetricsCollection": false
							},
							"enableStaging": true,
							"stagingSettings": {
								"linkedServiceName": {
									"referenceName": "TripFaresDataLakeStorageLinkedService",
									"type": "LinkedServiceReference",
									"parameters": {
										"keyVaultName": {
											"value": "@pipeline().parameters.KeyVaultName",
											"type": "Expression"
										},
										"datalakeAccountName": {
											"value": "@pipeline().parameters.datalakeAccountName",
											"type": "Expression"
										}
									}
								}
							}
						},
						"inputs": [
							{
								"referenceName": "faresDataSource",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "AzureSynapseAnalyticsFaresData",
								"type": "DatasetReference",
								"parameters": {
									"SynapseWorkspaceName": {
										"value": "@pipeline().parameters.SynapseWorkspaceName",
										"type": "Expression"
									},
									"SQLDedicatedPoolName": {
										"value": "@pipeline().parameters.SQLDedicatedPoolName",
										"type": "Expression"
									},
									"keyVaultName": {
										"value": "@pipeline().parameters.KeyVaultName",
										"type": "Expression"
									},
									"SQLLoginUsername": {
										"value": "@pipeline().parameters.SQLLoginUsername",
										"type": "Expression"
									}
								}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"parameters": {
					"SchemaName": {
						"type": "string",
						"defaultValue": "tripFares"
					},
					"SynapseWorkspaceName": {
						"type": "string",
						"defaultValue": "jhs2gt2acq3getmwpocws1.sql.azuresynapse.net"
					},
					"SQLDedicatedPoolName": {
						"type": "string",
						"defaultValue": "jhs2gt2acq3getmwpocws1p1"
					},
					"SQLLoginUsername": {
						"type": "string",
						"defaultValue": "jherna01"
					},
					"KeyVaultName": {
						"type": "string",
						"defaultValue": "kvjhs2gt2acq3getmwpoc"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "jhs2gt2acq3getmwpoc"
					}
				},
				"folder": {
					"name": "TripFaresDataPipeline"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripsDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSource')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/dataflows/tripFaresDataTransformations')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsSchema')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsTripsData')]",
				"[concat(variables('workspaceId'), '/datasets/AzureSynapseAnalyticsFaresData')]",
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsFaresData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "FaresData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTable2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureSynapseAnalytics2",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"tablename": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [
					{
						"name": "Employee",
						"type": "varchar"
					},
					{
						"name": "Salary",
						"type": "int",
						"precision": 10
					},
					{
						"name": "BeginDate",
						"type": "date"
					},
					{
						"name": "EndDate",
						"type": "date"
					},
					{
						"name": "CurrentRecord",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": {
						"value": "@dataset().tablename",
						"type": "Expression"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureSynapseAnalytics2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalyticsTripsData')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"table": "TripsData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SqlPoolTableEmployee')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "SqlPoolTable",
				"schema": [
					{
						"name": "Employee",
						"type": "varchar"
					},
					{
						"name": "Salary",
						"type": "int",
						"precision": 10
					},
					{
						"name": "BeginDate",
						"type": "date"
					},
					{
						"name": "EndDate",
						"type": "date"
					},
					{
						"name": "CurrentRecord",
						"type": "int",
						"precision": 10
					}
				],
				"typeProperties": {
					"schema": "dbo",
					"table": "DELTA_Employees"
				},
				"sqlPool": {
					"referenceName": "jhs2gt2acq3getmwpocws1p1",
					"type": "SqlPoolReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sqlPools/jhs2gt2acq3getmwpocws1p1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsSchema')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/azureSynapseAnalyticsTable')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresSynapseAnalyticsLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"SynapseWorkspaceName": {
							"value": "@dataset().SynapseWorkspaceName",
							"type": "Expression"
						},
						"SQLDedicatedPoolName": {
							"value": "@dataset().SQLDedicatedPoolName",
							"type": "Expression"
						},
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"SQLLoginUsername": {
							"value": "@dataset().SQLLoginUsername",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"SchemaName": {
						"type": "string"
					},
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "AzureSqlDWTable",
				"schema": [],
				"typeProperties": {
					"schema": {
						"value": "@dataset().SchemaName",
						"type": "Expression"
					},
					"table": "AggregateTaxiData"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresSynapseAnalyticsLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"keyVaultName": {
						"type": "string",
						"defaultValue": "kvmsft"
					},
					"datalakeAccountName": {
						"type": "string",
						"defaultValue": "adlsmsft"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "fares-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "payment_type",
						"type": "String"
					},
					{
						"name": "fare_amount",
						"type": "String"
					},
					{
						"name": "surcharge",
						"type": "String"
					},
					{
						"name": "mta_tax",
						"type": "String"
					},
					{
						"name": "tip_amount",
						"type": "String"
					},
					{
						"name": "tolls_amount",
						"type": "String"
					},
					{
						"name": "total_amount",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/faresDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripFareDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripFareDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripDataSink')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "TripFaresDataLakeStorageLinkedService",
					"type": "LinkedServiceReference",
					"parameters": {
						"keyVaultName": {
							"value": "@dataset().keyVaultName",
							"type": "Expression"
						},
						"datalakeAccountName": {
							"value": "@dataset().datalakeAccountName",
							"type": "Expression"
						}
					}
				},
				"parameters": {
					"datalakeAccountName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					}
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "trip-data.csv",
						"fileSystem": "public"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "medallion",
						"type": "String"
					},
					{
						"name": "hack_license",
						"type": "String"
					},
					{
						"name": "vendor_id",
						"type": "String"
					},
					{
						"name": "rate_code",
						"type": "String"
					},
					{
						"name": "store_and_fwd_flag",
						"type": "String"
					},
					{
						"name": "pickup_datetime",
						"type": "String"
					},
					{
						"name": "dropoff_datetime",
						"type": "String"
					},
					{
						"name": "passenger_count",
						"type": "String"
					},
					{
						"name": "trip_time_in_secs",
						"type": "String"
					},
					{
						"name": "trip_distance",
						"type": "String"
					},
					{
						"name": "pickup_longitude",
						"type": "String"
					},
					{
						"name": "pickup_latitude",
						"type": "String"
					},
					{
						"name": "dropoff_longitude",
						"type": "String"
					},
					{
						"name": "dropoff_latitude",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/TripFaresDataLakeStorageLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripsDataSource')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServerTripDataLinkedService",
					"type": "LinkedServiceReference"
				},
				"folder": {
					"name": "TripFareDatasets"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServerTripDataLinkedService')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSqlDatabase1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('AzureSqlDatabase1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalytics1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('AzureSynapseAnalytics1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureSynapseAnalytics2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('AzureSynapseAnalytics2_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServerTripFareDataLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServerTripFareDataLinkedService_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PowerBIWorkspaceTripsFares')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "PowerBIWorkspace",
				"typeProperties": {
					"workspaceID": "",
					"tenantID": "72f988bf-86f1-41af-91ab-2d7cd011db47"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresDataLakeStorageLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					},
					"datalakeAccountName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('TripFaresDataLakeStorageLinkedService_properties_typeProperties_url')]",
					"accountKey": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "adlsAccessKey"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TripFaresSynapseAnalyticsLinkedService')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"SynapseWorkspaceName": {
						"type": "string"
					},
					"SQLDedicatedPoolName": {
						"type": "string"
					},
					"keyVaultName": {
						"type": "string"
					},
					"SQLLoginUsername": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('TripFaresSynapseAnalyticsLinkedService_connectionString')]",
					"password": {
						"type": "AzureKeyVaultSecret",
						"store": {
							"referenceName": "keyVaultLinkedservice",
							"type": "LinkedServiceReference",
							"parameters": {
								"keyVaultName": {
									"value": "@linkedService().keyVaultName",
									"type": "Expression"
								}
							}
						},
						"secretName": "synapseSqlLoginPassword"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]",
				"[concat(variables('workspaceId'), '/linkedServices/keyVaultLinkedservice')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/adlsworspace')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('adlsworspace_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('adlsworspace_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/keyVaultLinkedservice')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"keyVaultName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('keyVaultLinkedservice_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/linkedService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('linkedService1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('linkedService1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "IntegrationRuntime1",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/IntegrationRuntime1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": true
						},
						"pipelineExternalComputeScaleProperties": {
							"timeToLive": 60
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IntegrationRuntime1')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0,
							"cleanup": false
						}
					}
				},
				"managedVirtualNetwork": {
					"type": "ManagedVirtualNetworkReference",
					"referenceName": "default"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/managedVirtualNetworks/default')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CopyDeltatoSQL')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "linkedService1",
								"type": "LinkedServiceReference"
							},
							"name": "DeltaTable"
						}
					],
					"sinks": [
						{
							"linkedService": {
								"referenceName": "AzureSynapseAnalytics2",
								"type": "LinkedServiceReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"parameters{",
						"     adf_tablename as string",
						"}",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'delta',",
						"     fileSystem: 'dlsjhspocfs1',",
						"     folderPath: 'DELTA_Employees/Employees') ~> DeltaTable",
						"DeltaTable sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     format: 'table',",
						"     store: 'synapseanalytics',",
						"     schemaName: 'dbo',",
						"     tableName: ($adf_tablename),",
						"     insertable: true,",
						"     updateable: false,",
						"     deletable: false,",
						"     upsertable: false,",
						"     staged: true,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError') ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/linkedService1')]",
				"[concat(variables('workspaceId'), '/linkedServices/AzureSynapseAnalytics2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tripFaresDataTransformations')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "TripFaresDataFlow"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "tripDataSink",
								"type": "DatasetReference"
							},
							"name": "TripDataCSV"
						},
						{
							"dataset": {
								"referenceName": "faresDataSink",
								"type": "DatasetReference"
							},
							"name": "FaresDataCSV"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "azureSynapseAnalyticsTable",
								"type": "DatasetReference"
							},
							"name": "SynapseAnalyticsSink"
						}
					],
					"transformations": [
						{
							"name": "AggregateByPaymentType"
						},
						{
							"name": "InnerJoinWithTripFares"
						}
					],
					"script": "source(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\trate_code as string,\n\t\tstore_and_fwd_flag as string,\n\t\tpickup_datetime as string,\n\t\tdropoff_datetime as string,\n\t\tpassenger_count as string,\n\t\ttrip_time_in_secs as string,\n\t\ttrip_distance as string,\n\t\tpickup_longitude as string,\n\t\tpickup_latitude as string,\n\t\tdropoff_longitude as string,\n\t\tdropoff_latitude as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> TripDataCSV\nsource(output(\n\t\tmedallion as string,\n\t\thack_license as string,\n\t\tvendor_id as string,\n\t\tpickup_datetime as string,\n\t\tpayment_type as string,\n\t\tfare_amount as string,\n\t\tsurcharge as string,\n\t\tmta_tax as string,\n\t\ttip_amount as string,\n\t\ttolls_amount as string,\n\t\ttotal_amount as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tinferDriftedColumnTypes: true,\n\tignoreNoFilesFound: false) ~> FaresDataCSV\nInnerJoinWithTripFares aggregate(groupBy(payment_type),\n\taverage_fare = avg(toInteger(total_amount)),\n\t\ttotal_trip_distance = sum(toInteger(trip_distance))) ~> AggregateByPaymentType\nTripDataCSV, FaresDataCSV join(TripDataCSV@medallion == FaresDataCSV@medallion\n\t&& TripDataCSV@hack_license == FaresDataCSV@hack_license\n\t&& TripDataCSV@vendor_id == FaresDataCSV@vendor_id\n\t&& TripDataCSV@pickup_datetime == FaresDataCSV@pickup_datetime,\n\tjoinType:'inner',\n\tmatchType:'exact',\n\tignoreSpaces: false,\n\tbroadcast: 'auto')~> InnerJoinWithTripFares\nAggregateByPaymentType sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tstaged: false,\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true,\n\terrorHandlingOption: 'stopOnFirstError') ~> SynapseAnalyticsSink"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/tripDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/faresDataSink')]",
				"[concat(variables('workspaceId'), '/datasets/azureSynapseAnalyticsTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP 20 QT.query_sql_text, SP.query_plan,\n\tRS.min_duration/1000000 as min_duration_sec, \n\tRS.avg_duration/1000000 as avg_duration_sec, \n\tRS.[max_duration]/1000000 as max_duration_sec,\n\tRS.min_tempdb_space_used * 8192 / 1024 / 1024 as min_tempdb_MB,\n\tRS.avg_tempdb_space_used * 8192 / 1024 / 1024 as avg_tempdb_MB,\n\tRS.max_tempdb_space_used * 8192 / 1024 / 1024 as max_tempdb_MB\nFROM sys.query_store_query SQ\nINNER JOIN sys.query_store_query_text QT \n\tON SQ.query_text_id = QT.query_text_id\nINNER JOIN sys.query_store_plan SP \n\tON SQ.query_id = SP.query_id\nINNER JOIN sys.query_store_runtime_stats RS \n\tON SP.plan_id = RS.plan_id\nORDER BY avg_duration DESC\n\nselect * from sys.query_context_settings\n\nSELECT current_storage_size_mb, max_storage_size_mb\nFROM sys.database_query_store_options;\n\nALTER DATABASE jhs2gt2acq3getmwpocws1p1\nSET QUERY_STORE (\n    OPERATION_MODE = READ_WRITE,\n    CLEANUP_POLICY = (STALE_QUERY_THRESHOLD_DAYS = 30),\n    DATA_FLUSH_INTERVAL_SECONDS = 3000,\n    MAX_STORAGE_SIZE_MB = 500,\n    INTERVAL_LENGTH_MINUTES = 15,\n    SIZE_BASED_CLEANUP_MODE = AUTO,\n    QUERY_CAPTURE_MODE = AUTO,\n    MAX_PLANS_PER_QUERY = 1000,\n    WAIT_STATS_CAPTURE_MODE = ON\n);\n\nSELECT name, is_auto_create_stats_on\nFROM sys.databases\n\n-- Monitor active queries\nSELECT *\nFROM sys.dm_pdw_exec_requests\nWHERE status not in ('Completed','Failed','Cancelled')\n  AND session_id <> session_id()\nORDER BY submit_time DESC;\n\n-- Find top 10 queries longest running queries\nSELECT TOP 10 *\nFROM sys.dm_pdw_exec_requests\nORDER BY total_elapsed_time DESC;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 10')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP TABLE IF EXISTS dbo.Student1;\n\nCREATE TABLE Student\n(\n   [Name]     VARCHAR(50)\n  ,[Maths]    INT\n  ,[Science]  INT\n  ,[English]  INT\n);\n    \nINSERT INTO Student\nVALUES\n    ('Atul', 'Atul2',     90, 91, 80, 81, 70, 71);\nINSERT INTO Student\nVALUES\n    ('Vishal','Vishal2',  80, 81, 90, 91, 60, 61);\nINSERT INTO Student\nVALUES\n    ('Shailesh', 'Shailesh2', 95, 96, 85, 86, 99, 100);\nGO\n\nSELECT * FROM Student; \n\n\nSELECT Name1, case when Indicador= 'Maths1'then 1\n                when Indicador='Science1' then 2\n                else 3\n                END as Indicador, Valor1 \nFROM   \n    (SELECT Name1, Maths1,  Science1, English1 FROM Student) p\nUNPIVOT\n(\n  Valor1\n  FOR Indicador in (Maths1, Science1, English1)\n) Upt\n;\n\nSELECT Name1, Name2, Indicador, Valor1, Valor2\nfrom Student\n    cross APPLY (VALUES(Maths1, '1', Maths2, '1'),\n                        (Science1, '2',Science2 '2'),\n                        (English1, '3', English2 '3')\n                        ) unpt (Valor1, Valor2, Indicador);\n\nSELECT s.Name1, s.Name2, unpt.Indicador, unpt.Valor1 \nfrom Student as s\n    cross APPLY (values(s.Maths1, '1'),\n                        (s.Science1, '2'),\n                        (s.English1, '3'))\n                         unpt (Valor1, Indicador);\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 11')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "DROP TABLE [dbo].[Student]\nGO\n\nSET ANSI_NULLS ON\nGO\nSET QUOTED_IDENTIFIER ON\nGO\n\nCREATE TABLE [dbo].[Student]\n( \n\t[Name1] [varchar](50)  NULL,\n\t[Name2] [varchar](50)  NULL,\n\t[Maths1] [int]  NULL,\n\t[Maths2] [int]  NULL,\n\t[Science1] [int]  NULL,\n\t[Science2] [int]  NULL,\n\t[English1] [int]  NULL,\n\t[English2] [int]  NULL\n)\nWITH\n(\n\tDISTRIBUTION = ROUND_ROBIN,\n\tCLUSTERED COLUMNSTORE INDEX\n)\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [context_settings_id]\n,[set_options]\n,[language_id]\n,[date_format]\n,[date_first]\n,[status]\n,[required_cursor_options]\n,[acceptable_cursor_options]\n,[merge_action_type]\n,[default_schema_id]\n,[is_replication_specific]\n,[is_contained]\n FROM [sys].[query_context_settings]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [plan_id]\n,[query_id]\n,[plan_group_id]\n,[engine_version]\n,[compatibility_level]\n,[query_plan_hash]\n,[query_plan]\n,[is_online_index_plan]\n,[is_trivial_plan]\n,[is_parallel_plan]\n,[is_forced_plan]\n,[is_natively_compiled]\n,[force_failure_count]\n,[last_force_failure_reason]\n,[last_force_failure_reason_desc]\n,[count_compiles]\n,[initial_compile_start_time]\n,[last_compile_start_time]\n,[last_execution_time]\n,[avg_compile_duration]\n,[last_compile_duration]\n,[plan_forcing_type]\n,[plan_forcing_type_desc]\n FROM [sys].[query_store_plan]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 4')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [runtime_stats_id]\n,[plan_id]\n,[runtime_stats_interval_id]\n,[execution_type]\n,[execution_type_desc]\n,[first_execution_time]\n,[last_execution_time]\n,[count_executions]\n,[avg_duration]\n,[last_duration]\n,[min_duration]\n,[max_duration]\n,[stdev_duration]\n,[avg_cpu_time]\n,[last_cpu_time]\n,[min_cpu_time]\n,[max_cpu_time]\n,[stdev_cpu_time]\n,[avg_logical_io_reads]\n,[last_logical_io_reads]\n,[min_logical_io_reads]\n,[max_logical_io_reads]\n,[stdev_logical_io_reads]\n,[avg_logical_io_writes]\n,[last_logical_io_writes]\n,[min_logical_io_writes]\n,[max_logical_io_writes]\n,[stdev_logical_io_writes]\n,[avg_physical_io_reads]\n,[last_physical_io_reads]\n,[min_physical_io_reads]\n,[max_physical_io_reads]\n,[stdev_physical_io_reads]\n,[avg_clr_time]\n,[last_clr_time]\n,[min_clr_time]\n,[max_clr_time]\n,[stdev_clr_time]\n,[avg_dop]\n,[last_dop]\n,[min_dop]\n,[max_dop]\n,[stdev_dop]\n,[avg_query_max_used_memory]\n,[last_query_max_used_memory]\n,[min_query_max_used_memory]\n,[max_query_max_used_memory]\n,[stdev_query_max_used_memory]\n,[avg_rowcount]\n,[last_rowcount]\n,[min_rowcount]\n,[max_rowcount]\n,[stdev_rowcount]\n,[avg_num_physical_io_reads]\n,[last_num_physical_io_reads]\n,[min_num_physical_io_reads]\n,[max_num_physical_io_reads]\n,[stdev_num_physical_io_reads]\n,[avg_log_bytes_used]\n,[last_log_bytes_used]\n,[min_log_bytes_used]\n,[max_log_bytes_used]\n,[stdev_log_bytes_used]\n,[avg_tempdb_space_used]\n,[last_tempdb_space_used]\n,[min_tempdb_space_used]\n,[max_tempdb_space_used]\n,[stdev_tempdb_space_used]\n,[avg_page_server_io_reads]\n,[last_page_server_io_reads]\n,[min_page_server_io_reads]\n,[max_page_server_io_reads]\n,[stdev_page_server_io_reads]\n FROM [sys].[query_store_runtime_stats]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [runtime_stats_interval_id]\n,[start_time]\n,[end_time]\n,[comment]\n FROM [sys].[query_store_runtime_stats_interval]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [query_id]\n,[query_text_id]\n,[context_settings_id]\n,[object_id]\n,[batch_sql_handle]\n,[query_hash]\n,[is_internal_query]\n,[query_parameterization_type]\n,[query_parameterization_type_desc]\n,[initial_compile_start_time]\n,[last_compile_start_time]\n,[last_execution_time]\n,[last_compile_batch_sql_handle]\n,[last_compile_batch_offset_start]\n,[last_compile_batch_offset_end]\n,[count_compiles]\n,[avg_compile_duration]\n,[last_compile_duration]\n,[avg_bind_duration]\n,[last_bind_duration]\n,[avg_bind_cpu_time]\n,[last_bind_cpu_time]\n,[avg_optimize_duration]\n,[last_optimize_duration]\n,[avg_optimize_cpu_time]\n,[last_optimize_cpu_time]\n,[avg_compile_memory_kb]\n,[last_compile_memory_kb]\n,[max_compile_memory_kb]\n,[is_clouddb_internal_query]\n FROM [sys].[query_store_query]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 7')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM ",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "default",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 8')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [Employee]\n,[Salary]\n,[BeginDate]\n,[EndDate]\n,[CurrentRecord]\n FROM [dbo].[DELTA_Employees]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "jhs2gt2acq3getmwpocws1p1",
						"poolName": "jhs2gt2acq3getmwpocws1p1"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 9')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://jhs2gt2acq3getmwpoc.dfs.core.windows.net/dlsjhspocfs1/DELTA_Employees/Employees/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06 Delta Lake Demo Full')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cf27c913-2bb4-49e1-bb56-d45730377adc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Using Linux foundation Delta Lake in Synapse Spark\n",
							"In this notebook, how to read the delta table, how to write to delta table and timetravel is demonstrated"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Set the strorage path info\n",
							"account_name = 'jhs2gt2acq3getmwpoc' # fill in your primary storage account name\n",
							"container_name = 'dlsjhspocfs1' # fill in your container name\n",
							"relative_csv_path = 'source' # fill in your relative CSV folder path\n",
							"relative_delta_path='DELTA_HRData' # fill in your relative delta lake folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/' % (container_name, account_name)\n",
							"print('Primary storage account path: ' + adls_path)\n",
							"\n",
							"#csv input file path\n",
							"csvfilepath = adls_path + relative_csv_path + '/00 HRData.csv'\n",
							"print ('CSV file path: '+ csvfilepath)\n",
							"\n",
							"# Delta Lake relative path\n",
							"deltatablepath = adls_path + relative_delta_path + '/'\n",
							"print('Delta Lake path: ' + deltatablepath)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Read data in csv format\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"csvhrdatadf = spark.read.option(\"header\",True).format(\"csv\").load(csvfilepath)\n",
							"csvhrdatadf.show(10)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Write data in delta format\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"csvhrdatadf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Department\").save(deltatablepath)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_hr = spark.read.format(\"delta\").load(deltatablepath)\n",
							"df_hr.show(10)\n",
							""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Check version with timetravel\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"deltaTable = DeltaTable.forPath(spark,deltatablepath)\r\n",
							"display(deltaTable.history())"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Update records that match the given condition \n",
							"Lets update here PayRate for employees whose payroll is less than 20 to make it lowest payrate value above 20.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"minPayRateAbove20 = df_hr.filter(\"PayRate>20\").agg({\"PayRate\":\"min\"}).collect()[0][\"min(PayRate)\"]\n",
							"\n",
							"#Number of records that will be updated\n",
							"deltaTable.toDF().filter(col(\"PayRate\")<minPayRateAbove20).count()\n",
							""
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Update PayRate to 20.5 for employees whose PayRate is below 20 \n",
							"deltaTable.update(\n",
							"    condition = (col(\"PayRate\")<minPayRateAbove20),\n",
							"    set = {\"PayRate\":minPayRateAbove20}\n",
							")\n",
							"deltaTable.toDF().filter(col(\"PayRate\")<minPayRateAbove20).show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Validate changes by filtering records on condition\n",
							"Validate no employees have PayRate less than or equal to 20\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"deltaTableAfterUpdate = DeltaTable.forPath(spark,deltatablepath)\n",
							"deltaTableAfterUpdate.toDF().filter(col(\"PayRate\")<minPayRateAbove20).count()"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Audit data changes\n",
							"First Check Version history\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(deltaTable.history())"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Undo changes for DeltaTable by restoring previous version\n",
							"Lets set the PayRate as it was in previous version "
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#Get verison 0 details\n",
							"hrdataversion0 = spark\\\n",
							"                        .read\\\n",
							"                        .format(\"delta\")\\\n",
							"                        .option(\"versionAsOF\",0)\\\n",
							"                        .load(deltatablepath)\n",
							"print(\"HR Dataframe as of version 0: \")\n",
							"hrdataversion0.show(10)\n",
							"\n",
							"print(\"In version 0 count of employees who have PayRate less than or equal to 20 are:%d\" % hrdataversion0.filter(col(\"PayRate\")<minPayRateAbove20).count())\n",
							"\n",
							"\n",
							"# Revert changes\n",
							"hrdataversion0.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Department\").save(deltatablepath)\n",
							"\n",
							"#read data and check count of employees again\n",
							"finalversion = spark.read.format(\"delta\").load(deltatablepath)\n",
							"print(\"In latest version count of employees who have PayRate less than or equal to 20 are: %d\" % finalversion.filter(col(\"PayRate\")<minPayRateAbove20).count())\n",
							"finalversion.show(10)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(deltaTable.history())"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Now work wiht Spark SQL"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM delta.`{}` LIMIT 5\".format(deltatablepath)))"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"\"\"\r\n",
							"  DROP TABLE IF EXISTS hr_delta\r\n",
							"\"\"\")\r\n",
							"spark.sql(\"\"\"\r\n",
							"  CREATE TABLE hr_delta\r\n",
							"  USING DELTA\r\n",
							"  LOCATION '{}'\r\n",
							"\"\"\".format(deltatablepath))"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT sum(payrate) FROM hr_delta"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT count(*) FROM hr_delta"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DESCRIBE history hr_delta"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#csv input file path\r\n",
							"updatecsvfilepath = adls_path + relative_csv_path + '/01 HRData.csv'\r\n",
							"newDataDF = spark.read.option(\"header\",True).format(\"csv\").load(updatecsvfilepath)\r\n",
							"newDataDF.show(10)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"newDataDF.count()"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"(newDataDF\r\n",
							"  .write\r\n",
							"  .format(\"delta\")\r\n",
							"  .partitionBy(\"Department\")\r\n",
							"  .mode(\"append\")\r\n",
							"  .save(deltatablepath)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT sum(payrate) FROM hr_delta;\r\n",
							"SELECT count(*) FROM hr_delta;"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DESCRIBE history hr_delta"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/06 Delta Lake Demo')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d048110c-c3d8-4b3f-90e5-90e363728873"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Using Linux foundation Delta Lake in Synapse Spark\n",
							"In this notebook, how to read the delta table, how to write to delta table and timetravel is demonstrated"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Set the strorage path info\n",
							"account_name = 'jhs2gt2acq3getmwpoc' # fill in your primary storage account name\n",
							"container_name = 'dlsjhspocfs1' # fill in your container name\n",
							"relative_csv_path = 'source' # fill in your relative CSV folder path\n",
							"relative_delta_path='DELTA_HRData' # fill in your relative delta lake folder path\n",
							"\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/' % (container_name, account_name)\n",
							"print('Primary storage account path: ' + adls_path)\n",
							"\n",
							"#csv input file path\n",
							"csvfilepath = adls_path + relative_csv_path + '/00 HRData.csv'\n",
							"print ('CSV file path: '+ csvfilepath)\n",
							"\n",
							"# Delta Lake relative path\n",
							"deltatablepath = adls_path + relative_delta_path + '/'\n",
							"print('Delta Lake path: ' + deltatablepath)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Read data in csv format\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"csvhrdatadf = spark.read.option(\"header\",True).format(\"csv\").load(csvfilepath)\n",
							"csvhrdatadf.show(10)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Write data in delta format\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"csvhrdatadf.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Department\").save(deltatablepath)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_hr = spark.read.format(\"delta\").load(deltatablepath)\n",
							"df_hr.show(10)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"dfWithDate =  F.to_date(F.to_timestamp(col(\"YearsOfService\"), \"M/d/yyyy H:mm\"))\r\n",
							"\r\n",
							"dfWithDate.show()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Add New Column YearsOfService\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import pyspark.sql.functions as f\n",
							"\n",
							"df_hr_service = df_hr.withColumn('YearsOfService',2020-f.year(f.to_timestamp('DateofHire', 'MM/dd/yyyy H:mm')))\n",
							"df_hr_service.show(5)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Overwrite the entire delta table\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_hr_service.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Department\").option(\"mergeSchema\", True).save(deltatablepath)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Validate delta table is updated with new column\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"hrdataframe = spark.read.format(\"delta\").load(deltatablepath)\n",
							"hrdataframe.show(10)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Check version with timetravel\n",
							"\n",
							"we can see here yearsOfService column is not present in original delta table"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"hrdataoriginal = (spark\n",
							"                    .read\n",
							"                    .format(\"delta\")\n",
							"                    .option(\"versionAsOf\",0)\n",
							"                    .load(deltatablepath)\n",
							"                    )\n",
							"hrdataoriginal.show(10)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Update records that match the given condition \n",
							"Lets update here PayRate for employees whose payroll is less than 20 to make it lowest payrate value above 20.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql.functions import *\n",
							"from delta.tables import *\n",
							"\n",
							"deltaTable = DeltaTable.forPath(spark,deltatablepath)\n",
							"\n",
							"minPayRateAbove20 = hrdataoriginal.filter(\"PayRate>20\").agg({\"PayRate\":\"min\"}).collect()[0][\"min(PayRate)\"]\n",
							"\n",
							"#Number of records that will be updated\n",
							"deltaTable.toDF().filter(col(\"PayRate\")<minPayRateAbove20).count()\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import *\r\n",
							"from delta.tables import *\r\n",
							"\r\n",
							"deltaTable = DeltaTable.forPath(spark,deltatablepath)\r\n",
							"\r\n",
							"minPayRateAbove20 = hrdataframe.filter(\"PayRate>20\").agg({\"PayRate\":\"min\"}).collect()[0][\"min(PayRate)\"]\r\n",
							"\r\n",
							"#Number of records that will be updated\r\n",
							"deltaTable.toDF().filter(col(\"PayRate\")<minPayRateAbove20).count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Update PayRate to 20.5 for employees whose PayRate is below 20 \n",
							"deltaTable.update(\n",
							"    condition = (col(\"PayRate\")<minPayRateAbove20),\n",
							"    set = {\"PayRate\":minPayRateAbove20}\n",
							")\n",
							"deltaTable.toDF().filter(col(\"PayRate\")<minPayRateAbove20).show()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Validate changes by filtering records on condition\n",
							"Validate no employees have PayRate less than or equal to 20\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"deltaTableAfterUpdate = DeltaTable.forPath(spark,deltatablepath)\n",
							"deltaTableAfterUpdate.toDF().filter(col(\"PayRate\")<minPayRateAbove20).count()"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"## Audit data changes\n",
							"or Check Version history\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#get version history\n",
							"deltaTable.history().show()"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(deltaTable.history(1))"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Undo changes for DeltaTable by restoring previous version\n",
							"Lets set the PayRate as it was in previous version "
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#Get verison 0 details\n",
							"hrdataversion0 = spark\\\n",
							"                        .read\\\n",
							"                        .format(\"delta\")\\\n",
							"                        .option(\"versionAsOF\",8)\\\n",
							"                        .load(deltatablepath)\n",
							"print(\"HR Dataframe as of version 0: \")\n",
							"hrdataversion0.show(10)\n",
							"\n",
							"#totalPayrate0= spark.sql(\"SELECT sum (payrate) FROM delta.`{}` WHERE payrate <=20\".format(deltatablepath)\n",
							"print(\"In version 0 count of employees who have PayRate less than or equal to 20 are:%d\" % hrdataversion0.filter(col(\"PayRate\")<minPayRateAbove20).agg({\"PayRate\":\"sum\"}).collect()[0][\"sum(PayRate)\"])\n",
							"\n",
							"\n",
							"# Revert changes\n",
							"hrdataversion0.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Department\").save(deltatablepath)\n",
							"\n",
							"#read data and check count of employees again\n",
							"finalversion = spark.read.format(\"delta\").load(deltatablepath)\n",
							"print(\"In latest version count of employees who have PayRate less than or equal to 20 are: %d\" % finalversion.filter(col(\"PayRate\")<minPayRateAbove20).agg({\"PayRate\":\"sum\"}).collect()[0][\"sum(PayRate)\"])\n",
							"finalversion.show(10)"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"display(spark.sql(\"SELECT * FROM delta.`{}` LIMIT 5\".format(deltatablepath)))"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"\"\"\r\n",
							"  DROP TABLE IF EXISTS hr_delta\r\n",
							"\"\"\")\r\n",
							"spark.sql(\"\"\"\r\n",
							"  CREATE TABLE hr_delta\r\n",
							"  USING DELTA\r\n",
							"  LOCATION '{}'\r\n",
							"\"\"\".format(deltatablepath))"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(\"\"\"\r\n",
							"  DROP TABLE IF EXISTS hr_delta\r\n",
							"\"\"\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT sum(payrate) FROM hr_delta"
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT count(*) FROM hr_delta"
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"DESCRIBE history hr_delta"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#csv input file path\r\n",
							"updatecsvfilepath = adls_path + relative_csv_path + '/01 HRData.csv'\r\n",
							"newDataDF = spark.read.option(\"header\",True).format(\"csv\").load(updatecsvfilepath)\r\n",
							"newDataDF.show(10)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"newDataDF.count()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"(newDataDF\r\n",
							"  .write\r\n",
							"  .format(\"delta\")\r\n",
							"  .partitionBy(\"Department\")\r\n",
							"  .mode(\"append\")\r\n",
							"  .save(deltatablepath)\r\n",
							")"
						],
						"outputs": [],
						"execution_count": 30
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "ws1sparkpool1",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "57f0d8fe-31ae-4c60-a8fb-2af9c17f2092"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/ws1sparkpool1",
						"name": "ws1sparkpool1",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ws1sparkpool1",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "2.4",
						"nodeCount": 5,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a tip or not.\n",
							"\n",
							" https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data¶ \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaLakeOptimize')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0b26de50-49ad-46f1-940f-4469aaa0fea4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_spark",
						"display_name": "scala"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 15
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Lake OPTIMIZE"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\r\n",
							"val dataPath = s\"/deltaoptimizetest/data-$sessionId\";\r\n",
							"\r\n",
							"val numFiles = 1000"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Data preparation"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"spark.range(50000000).map { _ =>\r\n",
							"    (scala.util.Random.nextInt(10000000).toLong, scala.util.Random.nextInt(1000000000), scala.util.Random.nextInt(2))\r\n",
							"}.toDF(\"colA\", \"colB\", \"colC\").repartition(numFiles).write.mode(\"overwrite\").format(\"delta\").partitionBy(\"colC\").save(dataPath)\r\n",
							"\r\n",
							"// 50M rows with random integers stored in numFiles * 2 parquet files in colC=0, colC=1 partition"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// spark.conf.get(\"spark.databricks.delta.optimize.maxFileSize\")\r\n",
							"spark.conf.get(\"spark.executor.instances\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Optimize dataset\r\n",
							"\r\n",
							"Optimize will compact small target files into larger files.\r\n",
							"Target files are\r\n",
							"- smaller than `spark.databricks.delta.optimize.maxFileSize` (default 1GB) AND\r\n",
							"- under the corresponding partitions if partition filter condition is specified.\r\n",
							"\r\n",
							"Optimize won't optimize the target files if\r\n",
							"- only one file in the partition\r\n",
							"- no candidate for compaction e.g. 2 files of 700MB & maxFileSize=1GB\r\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(spark.conf.get(\"spark.databricks.delta.optimize.maxFileSize\"))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"val dt = io.delta.tables.DeltaTable.forPath(dataPath)\r\n",
							"\r\n",
							"// optimize API supports filter condition for partitioned columns. Below will optimize the files under \"colC=1\" only.\r\n",
							"dt.optimize(\"colC = 1\").show\r\n",
							"// optimize API returns statistics in Dataframe"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// optimize all files\r\n",
							"dt.optimize().show"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Check performance without optimize"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"def measure(f : => Unit) {\r\n",
							"    val start = System.nanoTime\r\n",
							"    f\r\n",
							"    val durationInMS = (System.nanoTime - start) / 1000 / 1000\r\n",
							"    println(\"duration(ms): \" + durationInMS )\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Use version 0 for non-optimized data\r\n",
							"val df = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(dataPath)\r\n",
							"val agg = df.agg(avg(\"colA\"))\r\n",
							"measure(println(agg.collect))"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Check performance after optimize"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"// Use the latest version for optimized data\r\n",
							"val df = spark.read.format(\"delta\").load(dataPath)\r\n",
							"val agg = df.agg(avg(\"colA\"))\r\n",
							"measure(println(agg.collect))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Spark SQL support\r\n",
							"\r\n",
							"Note: partition filter condition does not support in Spark SQL yet."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"// This won't optimize anything since the table is already optimized.\r\n",
							"spark.sql(s\"OPTIMIZE delta.`$dataPath`\").show"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### zOrder - Example"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(s\"OPTIMIZE delta.`$dataPath` ZORDER BY (ColA)\").show"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Temp Area"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"from delta.tables import *\r\n",
							"dt = DeltaTable.forPath(spark, '/deltaoptimizetest/data-550226')\r\n",
							"display(dt.optimize(condition = \"colC = 1\"))\r\n",
							"\r\n",
							"# display(dt.optimize(condition = \"colC = 1\", zOrderBy = \"colA\"))\r\n",
							"# help(dt.optimize)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.sql(s\"CREATE TABLE sampledata USING DELTA LOCATION '$dataPath'\")"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"OPTIMIZE sampledata ZORDER BY (ColA)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "feafa851-6fbd-425d-a3fb-dcfc3e09d9c7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# Define column headers\r\n",
							"columns = [\"Employee\",\"Salary\"]\r\n",
							"\r\n",
							"# Define data for original dataframe\r\n",
							"empOriginal = [(\"Employee_1\",50000),(\"Employee_2\",55000)]\r\n",
							"\r\n",
							"# Define data for updates dataframe\r\n",
							"empUpdates = [(\"Employee_1\",50000),(\"Employee_2\",60000),(\"Employee_3\",55000)]\r\n",
							"\r\n",
							"# Create dataframe with orignial employee data\r\n",
							"dfOriginal = spark.createDataFrame(data = empOriginal,schema = columns)\r\n",
							"\r\n",
							"# Create dataframe with updated employee data\r\n",
							"dfUpdates = spark.createDataFrame(data = empUpdates,schema = columns)\r\n",
							"\r\n",
							"# Display dfOriginal\r\n",
							"dfOriginal.show()\r\n",
							"\r\n",
							"# Display dfUpdates\r\n",
							"dfUpdates.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create temp views of both the dfOriginal and dfUpdates dataframes \r\n",
							"# so that we can easily query them with Spark SQL later\r\n",
							"\r\n",
							"#dfOriginal\r\n",
							"dfOriginal.createOrReplaceTempView('Employee_Original')\r\n",
							"\r\n",
							"#dfUpdates\r\n",
							"dfUpdates.createOrReplaceTempView('Employee_Updates')"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- Create Delta Lake table, define schema and location\r\n",
							"CREATE TABLE DELTA_Employees (\r\n",
							"  Employee STRING NOT NULL,\r\n",
							"  Salary INT NOT NULL,\r\n",
							"  BeginDate DATE NOT NULL,\r\n",
							"  EndDate DATE NOT NULL,\r\n",
							"  CurrentRecord INT NOT NULL \r\n",
							")\r\n",
							"USING DELTA\r\n",
							"-- specify data lake folder location\r\n",
							"LOCATION '/DELTA_Employees/Employees/' "
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"-- Merge statement to handle upsert of dfOrginal dataset into DELTA_Employees table\r\n",
							"MERGE INTO delta_employees t\r\n",
							"USING (\r\n",
							"    SELECT emp.Employee AS MergeKey, emp.*\r\n",
							"    FROM Employee_Original AS emp\r\n",
							"\r\n",
							"    UNION ALL\r\n",
							"\r\n",
							"    SELECT NULL AS MergeKey, emp.*\r\n",
							"    FROM Employee_Original AS emp\r\n",
							"    JOIN delta_employees AS de ON emp.Employee = de.Employee\r\n",
							"    WHERE de.CurrentRecord = 1 AND emp.Salary <> de.Salary\r\n",
							") s\r\n",
							"ON t.Employee = s.MergeKey\r\n",
							"WHEN MATCHED AND t.CurrentRecord = 1 AND t.Salary <> s.Salary THEN\r\n",
							"  UPDATE SET t.CurrentRecord = 0, t.EndDate = (CURRENT_DATE)\r\n",
							"WHEN NOT MATCHED THEN\r\n",
							"  INSERT (Employee, Salary, BeginDate, EndDate, CurrentRecord) VALUES (s.Employee, s.Salary, CURRENT_DATE, '2999-12-31',1)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"SELECT *\r\n",
							"FROM DELTA_Employees\r\n",
							"ORDER BY Employee"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/deltalake_autocompact_output')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "65cab808-e2de-407e-a9fc-63e85af9bcfd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "scala"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/8af3f092-169b-4f76-8cb3-822bfc9a88db/resourceGroups/rg-1-click-poc/providers/Microsoft.Synapse/workspaces/jhs2gt2acq3getmwpocws1/bigDataPools/spark31",
						"name": "spark31",
						"type": "Spark",
						"endpoint": "https://jhs2gt2acq3getmwpocws1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delta Lake - Auto compaction"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": true
						},
						"source": [
							"val sessionId = scala.util.Random.nextInt(1000000)\r\n",
							"val dataPath = s\"/deltaautooptimizetest/data-$sessionId\";\r\n",
							"\r\n",
							"val numFiles = 100"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Auto compaction\r\n",
							"\r\n",
							"Auto compaction will automatically trigger OPTIMIZE operation after any update on the table.\r\n",
							"\r\n",
							"In order to reduce the overhead,\r\n",
							"- use 128MB as `maxFileSize` (`spark.microsoft.delta.autoCompact.maxFileSize`, default: 128MB) \r\n",
							"- triggered when there're more than 50 files in a directory. (`spark.microsoft.delta.autoCompact.minNumFiles`, default: 50)\r\n",
							"- optimize small files up to the maximum bytes config (`spark.microsoft.delta.autoCompact.maxCompactBytes`, default: 50GB)"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"spark.conf.set(\"spark.microsoft.delta.autoCompact.enabled\", \"false\")\r\n",
							"spark.range(500000).map { _ =>\r\n",
							"    (scala.util.Random.nextInt(10000000).toLong, scala.util.Random.nextInt(1000000000), scala.util.Random.nextInt(2))\r\n",
							"}.toDF(\"colA\", \"colB\", \"colC\").repartition(numFiles).write.mode(\"overwrite\").format(\"delta\").partitionBy(\"colC\").save(dataPath)\r\n",
							"// 50M rows with random integers stored in numFiles * 2 parquet files in colC=0, colC=1 partition"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"val df = spark.read.format(\"delta\").load(dataPath)\r\n",
							"println(df.inputFiles.length) // The number of files is 100 * 2 = 200"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// Enable auto compaction for all tables.\r\n",
							"spark.conf.set(\"spark.microsoft.delta.autoCompact.enabled\", \"true\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"// Append 1 row to trigger auto compact\r\n",
							"df.limit(1).write.mode(\"append\").format(\"delta\").save(dataPath)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"val df = spark.read.format(\"delta\").load(dataPath)\r\n",
							"println(df.inputFiles.length) // The result should be 2 as there are 2 partitions."
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "test",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://dlsjhspocfs1@jhs2gt2acq3getmwpoc.dfs.core.windows.net/test",
								"Properties": {
									"FormatType": "csv",
									"LinkedServiceName": "jhs2gt2acq3getmwpocws1-WorkspaceDefaultStorage"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "test",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "test"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "C1",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C2",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C3",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C4",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C5",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C6",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C7",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C8",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C9",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C10",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C11",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C12",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C13",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "C14",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 8000,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://public@jhs2gt2acq3getmwpoc.dfs.core.windows.net//trip-data.csv",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://public@jhs2gt2acq3getmwpoc.dfs.core.windows.net//trip-data.csv",
									"Properties": {
										"LinkedServiceName": "jhs2gt2acq3getmwpocws1-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws1sparkpool1')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 40,
					"minNodeCount": 3
				},
				"nodeCount": 5,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "2.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark31')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.1",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/jhs2gt2acq3getmwpocws1p1')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "southcentralus"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/default')]",
			"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks",
			"apiVersion": "2019-06-01-preview",
			"properties": {},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "06 Delta Lake Demo",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "06 Delta Lake Demo",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spark31",
								"type": "BigDataPoolReference"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/06 Delta Lake Demo')]",
				"[concat(variables('workspaceId'), '/bigDataPools/spark31')]"
			]
		}
	]
}